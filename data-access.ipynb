{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb32b567-7aa7-4c37-aa0e-0a7a1022f6f7",
   "metadata": {},
   "source": [
    "# Data Access on the Planetary Computer\n",
    "\n",
    "In this notebook, we'll take a whirlwind tour of accessing geospatial data in many flavors. A few things to note as we go through it:\n",
    "\n",
    "1. We'll be using cloud-friendly formats\n",
    "    - We'll stream the data directly from Blob Storage into memory. No downloading to local disk!\n",
    "2. We'll always start with the STAC API\n",
    "    - No need to remember URLs / paths in blob storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c459269e-08f2-4084-8f6c-b90a866d3c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import operator\n",
    "import functools\n",
    "import warnings\n",
    "\n",
    "import dask.distributed\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "import stackstac\n",
    "import numpy as np\n",
    "import geopandas\n",
    "import dask.dataframe\n",
    "import dask_geopandas\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "import pyproj\n",
    "import pdal\n",
    "import shapely.geometry\n",
    "import shapely.ops\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"pandas.Float64Index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22e07b",
   "metadata": {},
   "source": [
    "We'll make a `catalog` client to interact with the Planetary Computer's STAC API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23613829-d037-4e62-9e86-30b8d91b475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee2ffb7",
   "metadata": {},
   "source": [
    "And we'll make a local Dask \"cluster\" to do some computations in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0cd0a6-ca02-44c9-bebc-0beb15c44ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = dask.distributed.Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3098f2c-80d1-47ef-99ed-f50008dde279",
   "metadata": {},
   "source": [
    "## Raster Data\n",
    "\n",
    "Raster data is typically stored as Cloud Optimized GeoTIFF. Some examples include\n",
    "\n",
    "* Satellite imagery / aerial photography\n",
    "    - [Landsat C2-L2](https://planetarycomputer.microsoft.com/dataset/landsat-8-c2-l2)\n",
    "    - [Sentinel 2 L2A](https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a)\n",
    "    - [NAIP](https://planetarycomputer.microsoft.com/dataset/naip)\n",
    "* Land use / land cover\n",
    "    - [Esri / IO 10-Meter Land Cover](https://planetarycomputer.microsoft.com/dataset/io-lulc-9-class)\n",
    "    - [Land Cover of Canada](https://planetarycomputer.microsoft.com/dataset/nrcan-landcover)\n",
    "* Elevation\n",
    "    - [COP DEM](https://planetarycomputer.microsoft.com/dataset/cop-dem-glo-30)\n",
    "    - [NASADEM](https://planetarycomputer.microsoft.com/dataset/nasadem)\n",
    "* \"Derived variables\"\n",
    "    - [Chloris Biomass](https://planetarycomputer.microsoft.com/dataset/chloris-biomass)\n",
    "    - [HGB](https://planetarycomputer.microsoft.com/dataset/hgb)\n",
    "    - [HREA](https://planetarycomputer.microsoft.com/dataset/hrea)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5133c35a",
   "metadata": {},
   "source": [
    "Here we use the STAC API to search for Sentinel-2 scenes matching some spatio-temporal query. We're even able to query on additional properties in the STAC metadata, like the `cloud_cover`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3da4f5-b881-4e29-a2ec-516bde80cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = catalog.search(\n",
    "    bbox=[-122.28, 47.55, -121.96, 47.75],\n",
    "    datetime=\"2020-01-01/2020-12-31\",\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 25}},\n",
    ")\n",
    "\n",
    "items = search.get_all_items()\n",
    "print(len(items))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0003f75",
   "metadata": {},
   "source": [
    "Whenever you're working with the Planetary Computer, you need to \"sign\" your items / assets. This appends a read-only SAS token to the data, so that we can fetch the data from Blob Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e681e2-0e14-4429-91b8-f044de0ac616",
   "metadata": {},
   "outputs": [],
   "source": [
    "signed_items = planetary_computer.sign(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759459b",
   "metadata": {},
   "source": [
    "STAC is all about *metadata*. So these STAC items are just some objects with links to the actual data (COGs in this case). There are lots of ways to go from a URL to an image, or from a list of STAC items to an image. In this case, we'll use `stackstac` which lets you stack your STAC items into an `xarray.DataArray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b35f01-31e3-4552-81ad-22e5fea4bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = stackstac.stack(\n",
    "    signed_items,\n",
    "    assets=[\"B04\", \"B08\"],  # red, nir\n",
    "    resolution=100,\n",
    ").where(\n",
    "    lambda x: x > 0, other=np.nan\n",
    ")  # sentinel-2 uses 0 as nodata\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e08e7dc",
   "metadata": {},
   "source": [
    "Let's do a little computation: we'll compute NDVI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76149d5-00df-4b06-b523-14426cdebc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "red = data.sel(band=\"B04\")\n",
    "nir = data.sel(band=\"B08\")\n",
    "\n",
    "ndvi = (red - nir) / (red + nir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed498001",
   "metadata": {},
   "source": [
    "And we'll plot it for the first time slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b726a-0c37-4b20-b73f-51fc6dee4781",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ndvi.isel(time=0).persist()\n",
    "m = stackstac.show(x, range=(-0.9, 0.9))\n",
    "m.scroll_wheel_zoom = True\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a63aa-1849-4b56-a24a-59e550de3087",
   "metadata": {},
   "source": [
    "## Earth systems data\n",
    "\n",
    "These datasets are typically stored as Zarr or NetCDF.\n",
    "\n",
    "* Climate model output\n",
    "    - Terraclimate, gridMet, Daymet, NEX-GDDP-CMIP6\n",
    "* Reanalysis\n",
    "    - ERA5\n",
    "* Observations\n",
    "    - GPM IMERG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b8475e",
   "metadata": {},
   "source": [
    "In this example, we'll load up some data from Terraclimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f65557-c31c-4083-86f2-49e2baad2333",
   "metadata": {},
   "outputs": [],
   "source": [
    "terraclimate = catalog.get_collection(\"terraclimate\")\n",
    "asset = terraclimate.assets[\"zarr-https\"]\n",
    "\n",
    "\n",
    "store = fsspec.get_mapper(asset.href)\n",
    "ds = xr.open_zarr(store, **asset.extra_fields[\"xarray:open_kwargs\"])\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc9bba4",
   "metadata": {},
   "source": [
    "Again, we have a DataArray. We can select the last time slice and plot `tmax` for the globe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b308c2-36f8-4de6-96b2-85b0870ac47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_max_temp = ds.isel(time=-1)[\"tmax\"].coarsen(lat=8, lon=8).mean().load()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10), subplot_kw=dict(projection=ccrs.Robinson()))\n",
    "\n",
    "average_max_temp.plot(ax=ax, transform=ccrs.PlateCarree())\n",
    "ax.coastlines();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bf3bf0-e02b-41e9-bcc8-0724d3892cc3",
   "metadata": {},
   "source": [
    "## Operational forecast data\n",
    "\n",
    "The Planetary Computer also includes some operational weather forecast data. These are typically stored as Zarr or GRIB2. In this example we'll load some data from the ECMWF's Open Data program (using the staging API. It'll be available through the production API soon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641257c-7856-4874-a178-cc963cbfe4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer-staging.microsoft.com/api/stac/v1\"\n",
    ")\n",
    "search = staging_catalog.search(\n",
    "    collections=[\"ecmwf-forecast\"],\n",
    "    query={\n",
    "        \"ecmwf:stream\": {\"eq\": \"wave\"},\n",
    "        \"ecmwf:type\": {\"eq\": \"fc\"},\n",
    "        \"ecmwf:step\": {\"eq\": \"0h\"},\n",
    "    },\n",
    ")\n",
    "items = search.get_all_items()\n",
    "item = items[0]\n",
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaa9186",
   "metadata": {},
   "source": [
    "The GRIB2 file format isn't really cloud-friendly. We're working on that, but in the meantime we'll download the file to disk and load it from there. Again into an `xarray.DataArray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fcf890-b46d-4042-ac5f-c100cd5abd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = item.assets[\"data\"].href\n",
    "filename, _ = urllib.request.urlretrieve(url)\n",
    "\n",
    "ds = xr.open_dataset(filename, engine=\"cfgrib\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc234f",
   "metadata": {},
   "source": [
    "Let's make another plot, thise time for \"significant wave height\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189fbdf8-30bf-4e98-a4e8-4b3f50bcfd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = projection = ccrs.Robinson()\n",
    "fig, ax = plt.subplots(figsize=(16, 9), subplot_kw=dict(projection=projection))\n",
    "\n",
    "ds.swh.plot(ax=ax, transform=ccrs.PlateCarree());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a62d74",
   "metadata": {},
   "source": [
    "Or we can plot the join distribution of the \"mean wave period\" and \"significant wave height\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07ee38-5379-4519-ab1f-c8046cfdefda",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.jointplot(\n",
    "    x=ds.mwp.data.ravel(), y=ds.swh.data.ravel(), alpha=0.25, marker=\".\", height=12\n",
    ")\n",
    "grid.ax_joint.set(xlabel=\"Mean wave period\", ylabel=\"Significant wave height\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b691c28-1567-4d5d-ae8c-928d60c42766",
   "metadata": {},
   "source": [
    "## Tabular data\n",
    "\n",
    "These are typically stored in Apache Parquet, using the geoparquet standard where appropriate.\n",
    "\n",
    "- US Census\n",
    "- Forest Inventory and Analysis\n",
    "- gNATSGO tables\n",
    "\n",
    "In this example, we'll load up the Census 2020 Congressional District boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0262d410-a656-42d2-ba58-5637b778cdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = catalog.search(collections=[\"us-census\"])\n",
    "items = planetary_computer.sign(search.get_all_items())\n",
    "items = {x.id: x for x in items}\n",
    "item = items[\"2020-cb_2020_us_cd116_500k\"]\n",
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3dc4a",
   "metadata": {},
   "source": [
    "That STAC item as a link a Parquet file in Blob Storage. We'll load it up with geopandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21973fdd-6745-4f3e-9257-6506b68a1210",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset = item.assets[\"data\"]\n",
    "df = geopandas.read_parquet(\n",
    "    asset.href, storage_options=asset.extra_fields[\"table:storage_options\"]\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2698c54",
   "metadata": {},
   "source": [
    "And plot the districts for Maryland (with a state FIPS code of `24`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91af64e9-3365-48ed-b1e6-7ef075719df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "maryland = df[df.STATEFP == \"24\"].astype({\"GEOID\": \"category\"})\n",
    "maryland.explore(column=\"GEOID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610a9465",
   "metadata": {},
   "source": [
    "The largest datasets from this collection are at the census-block level. This datasets would be too large to load with pandas or geopandas, which wants all of the data to fit in RAM. So we'll use `dask_geopandas` to load in the geometries data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c0a2ce-8ede-4781-a4ad-36844dc65efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset = items[\"2020-census-blocks-geo\"].assets[\"data\"]\n",
    "\n",
    "geo = dask_geopandas.read_parquet(\n",
    "    asset.href,\n",
    "    storage_options=asset.extra_fields[\"table:storage_options\"],\n",
    ")\n",
    "geo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a26483",
   "metadata": {},
   "source": [
    "And `dask.dataframe` to load in the population data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65599a84-6ea6-437f-a891-d5d431519c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset = items[\"2020-census-blocks-population\"].assets[\"data\"]\n",
    "\n",
    "pop = dask.dataframe.read_parquet(\n",
    "    asset.href,\n",
    "    storage_options=asset.extra_fields[\"table:storage_options\"],\n",
    ")\n",
    "pop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10b21ae",
   "metadata": {},
   "source": [
    "We can join those together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d241308-838e-4d40-ac5c-4abfd9274336",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = geo.join(pop)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ca8221",
   "metadata": {},
   "source": [
    "Notice that all those operations were instant. Dask is (mostly) lazy, so it only evaluates when you ask it to.\n",
    "\n",
    "Theses census-block levels are actually parquet datasets (a folder of files) partitioned by state. So we can do things at a state-level without having to look at the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50af9950-52c5-4dde-8f45-58c5d7d80751",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = [x for x in geo.divisions if x.startswith(\"44\")][0]\n",
    "stop = \"4499\"\n",
    "\n",
    "ri = geo.loc[start:stop].compute()\n",
    "ri.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5dcbe6-e7a5-481d-ae38-9e8229c3f08f",
   "metadata": {},
   "source": [
    "## Point-cloud data\n",
    "\n",
    "Typically stored as COPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff871b-5346-4ffb-905b-eea6fdf3d340",
   "metadata": {},
   "outputs": [],
   "source": [
    "bean = {\"type\": \"Point\", \"coordinates\": [-87.623358, 41.8826812]}\n",
    "\n",
    "geom = shapely.geometry.shape(bean)\n",
    "\n",
    "utm = pyproj.crs.CRS.from_epsg(32616)  # UTM zone for Chicago\n",
    "wgs84 = pyproj.CRS(\"EPSG:4326\")\n",
    "\n",
    "project_dd_to_utm = pyproj.Transformer.from_crs(wgs84, utm, always_xy=True).transform\n",
    "project_utm_to_dd = pyproj.Transformer.from_crs(utm, wgs84, always_xy=True).transform\n",
    "\n",
    "utm_point = shapely.ops.transform(project_dd_to_utm, geom)\n",
    "window = utm_point.buffer(400)\n",
    "\n",
    "window_dd = shapely.ops.transform(project_utm_to_dd, window)\n",
    "\n",
    "df = geopandas.GeoDataFrame(geometry=[window_dd], crs=\"EPSG:4326\")\n",
    "\n",
    "df.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Bean\n",
    "bean = shapely.geometry.shape(\n",
    "    {\"type\": \"Point\", \"coordinates\": [-87.623358, 41.8826812]}\n",
    ").buffer(0.005)\n",
    "\n",
    "# The *test* API. Really bleeding-edge now.\n",
    "\n",
    "test_catalog = pystac_client.Client.open(\n",
    "    \"https://pct-apis-staging.westeurope.cloudapp.azure.com/stac/\"\n",
    ")\n",
    "\n",
    "search = test_catalog.search(collections=[\"3dep-lidar-copc\"], intersects=bean)\n",
    "ic = search.get_all_items()\n",
    "\n",
    "# Filter out for only the Cook County LiDAR collections\n",
    "cook = []\n",
    "for item in ic:\n",
    "    if \"Cook\" in item.id:\n",
    "        cook.append(item)\n",
    "\n",
    "signed = [planetary_computer.sign(i) for i in cook]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e1957-76d1-4c23-8bc8-71c7259ccd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_RESOLUTION = 2.0\n",
    "READ_RESOLUTION = 2.0\n",
    "polygon = window.wkt + f\" / EPSG:{utm.to_epsg()}\"\n",
    "\n",
    "readers = []\n",
    "for tile in signed:\n",
    "    url = tile.assets[\"data\"].href\n",
    "    reader = pdal.Reader.copc(\n",
    "        url, requests=3, resolution=READ_RESOLUTION, polygon=polygon\n",
    "    )\n",
    "    readers.append(reader)\n",
    "\n",
    "\n",
    "assign = pdal.Filter.assign(value=[\"Intensity = Intensity / 256\"])\n",
    "\n",
    "writer = pdal.Writer.gdal(\n",
    "    \"intensity.tif\",\n",
    "    resolution=OUTPUT_RESOLUTION,\n",
    "    dimension=\"Intensity\",\n",
    "    data_type=\"uint8\",\n",
    "    output_type=\"mean\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94f976-f263-4c47-9ff6-39f442e73111",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = None\n",
    "\n",
    "pipeline = functools.reduce(operator.or_, readers)\n",
    "pipeline |= assign | writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e03a31-1620-48ff-be7a-33bd70ab2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Use streaming mode at 1e6 points at a time. This\n",
    "# helps us conserve memory for pipelines that are streamable\n",
    "# check that with the pipeline.streamable property\n",
    "results = pipeline.execute_streaming(chunk_size=1000000)\n",
    "print(pipeline.log)\n",
    "\n",
    "# the last stage of our pipeline is the writer, and the 'dimension'\n",
    "# option on the writer is what we want to print\n",
    "dimension = pipeline.stages[-1].options[\"dimension\"]\n",
    "print(f\"Number of points returned for dimension {dimension}: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81ab909-ee2c-41a4-8b7f-a018d1e405a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.open(\"intensity.tif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
