{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c61d41b",
   "metadata": {},
   "source": [
    "# Crop Type Prediction\n",
    "\n",
    "This notebook trains a model to predict crop types from Sentinel 2 Level 2-A imagery.\n",
    "\n",
    "Our training labels come from the Radiant Earth [South Africa Crop Type Competition](https://registry.mlhub.earth/10.34911/rdnt.j0co8q/). They're a collection of scenes, with integers indicating the crop type at each pixel in the scene.\n",
    "\n",
    "Our training data comes from Microsoft's Planetary Computer. The [Sentinel 2 Level 2-A](https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a) page describes what all is avaiable.\n",
    "\n",
    "## Data access\n",
    "\n",
    "We'll use STAC for data access. Specifically, we'll interact with two STAC catalogs\n",
    "\n",
    "1. A static catalog for the labels, hosted in a Blob Storage container\n",
    "2. The Planetary Computer's STAC API, to query for scenes matching some condition\n",
    "\n",
    "The overall workflow will be\n",
    "\n",
    "1. Load a \"chip\" with the label data (a 256x256 array of integer codes indicate the crop type)\n",
    "2. Search for and load a scene with Sentinel 2 imagery covering the `labels` chip\n",
    "3. Transform and crop the (very large) Sentinel 2 scene to match the 256x256 label scene\n",
    "4. Stack and reshape the data for the machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f14c2a-7fb3-4791-958e-40d76b3bca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystac\n",
    "import pystac_client\n",
    "import requests\n",
    "import shapely.geometry\n",
    "import shapely.ops\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \"Creating an ndarray from ragged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f87bc8",
   "metadata": {},
   "source": [
    "### Load labels\n",
    "\n",
    "We have a STAC catalog of labels for the training data, which is based off the collection used in the Radiant Earth competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2951813",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_catalog = pystac.read_file(\n",
    "    \"https://esip2021.blob.core.windows.net/esip2021/train/collection.json\"\n",
    ")\n",
    "training_catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea99d2b3-ba8c-49c8-86ce-2fe6cde0bbd1",
   "metadata": {},
   "source": [
    "Each of these Items contains a few thing. They all share the same set of labels: integer codes indicating a particular crop type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c359e4-1f9a-4d44-94ab-69a4cbf2d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SCENES = 25\n",
    "links = training_catalog.get_item_links()[:N_SCENES]\n",
    "label_items = [link.resolve_stac_object().target for link in links]\n",
    "\n",
    "labels = requests.get(label_items[0].assets[\"raster_values\"].href).json()\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424f58f1-8791-48e7-aa9f-e52f3766383b",
   "metadata": {},
   "source": [
    "And like any STAC item, they have a specific footprint. Let's plot them on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b94b06e-4cc4-450a-8f5d-e68dae7cd3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "\n",
    "df = geopandas.GeoDataFrame.from_features([x.to_dict() for x in label_items]).set_crs(\n",
    "    4326\n",
    ")\n",
    "m = df.explore()\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a676b5-ccaf-450e-a290-c9e31c5a51f2",
   "metadata": {},
   "source": [
    "Each one of these plots is a (256 x 256) \"chip\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91765be9-eecd-4dfd-b1ac-639fa10fd604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "\n",
    "rioxarray.open_rasterio(label_items[9].assets[\"labels\"].href).squeeze().plot.imshow(\n",
    "    cmap=\"tab10\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9ba998-4ce7-40b9-a0cf-972d27e6757a",
   "metadata": {},
   "source": [
    "We need to associate the label items with a Sentinel-2 Level 2-A item. We need to find an item that (mostly) covers the field and isn't too cloudy.\n",
    "\n",
    "We could make one STAC query per label item, but that would be a bit slow and inefficient. Instead, we'll do one search to get all the items covering the bounding box of *all* of our fields. So we need the union of all the bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054aff6-971e-4bef-80d3-bf19bf6fe519",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = shapely.ops.unary_union(\n",
    "    [shapely.geometry.box(*item.bbox) for item in label_items]\n",
    ").bounds\n",
    "bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafbfb43-83eb-4b81-a5a9-7f43231fa983",
   "metadata": {},
   "source": [
    "Now we'll make a search for all the items matching our requirements, similar to the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6b9073-cc64-4a70-90d8-0fa3eedafe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "stac_client = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1/\"\n",
    ")\n",
    "\n",
    "date_range = \"2017-06-01/2017-09-01\"\n",
    "\n",
    "search = stac_client.search(\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    bbox=bbox,\n",
    "    datetime=date_range,\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 25}},\n",
    ")\n",
    "sentinel_items = list(search.get_all_items())\n",
    "len(sentinel_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336cbc89-dc45-4cdd-9e7f-dbd74681bf1e",
   "metadata": {},
   "source": [
    "So we have bunch of Sentinel 2 items that together cover all of our fields. But these Sentinel scenes are much larger than our fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499af7a6-0d41-4d0d-beb7-3aa792a673e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "sentinel_item = sentinel_items[1]\n",
    "\n",
    "layer = folium.TileLayer(\n",
    "    requests.get(sentinel_item.assets[\"tilejson\"].href).json()[\"tiles\"][0],\n",
    "    attr=\"Sentinel-2 L2A\",\n",
    ")\n",
    "\n",
    "layer.add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c86e4-ad3e-4ece-a084-9a178a6f61c4",
   "metadata": {},
   "source": [
    "How do we know which (part of a) Sentinel-2 scene goes with each field? That's what we do in the next section. It's a bit complicated, but the basic idea is to pick the least-cloudy Sentinel-2 scene that (mostly) covers our field (at least 90% of it anyway)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b8402-f169-4c51-8e6b-a33e5dfd56c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match(label_item, sentinel_items):\n",
    "    # make sure we pick a sentinel scene that overlaps substantially with the label\n",
    "    label_shape = shapely.geometry.shape(label_item.geometry)\n",
    "    items2 = [\n",
    "        item\n",
    "        for item in sentinel_items\n",
    "        if (\n",
    "            shapely.geometry.shape(item.geometry).intersection(label_shape).area\n",
    "            / label_shape.area\n",
    "        )\n",
    "        > 0.90\n",
    "    ]\n",
    "    sentinel_item = min(\n",
    "        items2, key=lambda item: pystac.extensions.eo.EOExtension.ext(item).cloud_cover\n",
    "    )\n",
    "    return sentinel_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc03fdf-6ff9-4244-9cdf-d0e37066a15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import planetary_computer\n",
    "\n",
    "matched = [\n",
    "    planetary_computer.sign(find_match(label_item, sentinel_items))\n",
    "    for label_item in label_items\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f92abcb-97d1-4075-9687-3b43065a1769",
   "metadata": {},
   "source": [
    "Given the matched `(label_item, sentinel_item)` pairs, we can load in the actual data. Like in the last notebook, we'll use `stackstac` to load a bunch of bands for the training data. We'll also load the label data at the same time.\n",
    "\n",
    "Finally, there's a slight pixel alignmnet issue, where the coordinates on the `label` data are shifted by a half-pixel from the coordinates for the training data. We'll shift the training data to match the label data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c02ff8d-d114-4303-9f96-4a6674418cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rioxarray\n",
    "import stackstac\n",
    "\n",
    "\n",
    "def load(label_item, sentinel_item):\n",
    "    label_data = rioxarray.open_rasterio(label_item.assets[\"labels\"].href).squeeze()\n",
    "\n",
    "    assets = [\"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B09\"]\n",
    "    data = (\n",
    "        stackstac.stack(\n",
    "            sentinel_item.to_dict(),\n",
    "            assets=assets,\n",
    "            epsg=label_data.rio.crs.to_epsg(),  # reproject to the labels' CRS\n",
    "            bounds=label_data.rio.bounds(),  # crop to the labels' bounds\n",
    "            resolution=10,  # resample all assets to the highest resolution\n",
    "            dtype=\"float32\",\n",
    "            fill_value=0,\n",
    "        )\n",
    "        .squeeze()\n",
    "        .assign_coords(\n",
    "            y=lambda ds: (ds.y - 5).round(),  # fix half-pixel label issue\n",
    "            x=lambda ds: (ds.x + 5).round(),\n",
    "        )\n",
    "        .compute()\n",
    "    )\n",
    "\n",
    "    assert data.shape[1:] == label_data.shape\n",
    "\n",
    "    # Add a label_id dimension, to track which training data goes with\n",
    "    # which pixels. This will be helpful later on in evaluation.\n",
    "    data = data.expand_dims({\"label_id\": [label_item.id]})\n",
    "    label_data = label_data.expand_dims({\"label_id\": [label_item.id]})\n",
    "\n",
    "    return data, label_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed08d1-3d32-4ca5-8f00-fc95ac7020ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"pandas.Float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c198e3-5b4e-4133-8537-44ba0ef7c60d",
   "metadata": {},
   "source": [
    "We're actually loading data now. This will take a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a76edb-1d41-422b-ab91-97dbe785c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Xs, ys = zip(\n",
    "    *[\n",
    "        load(label_item, sentinel_item)\n",
    "        for label_item, sentinel_item in zip(label_items, matched)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd21b7b-7b75-48b8-bcf3-6b77fb609a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35a4ef6-bbf6-45f9-894f-48a49a45b220",
   "metadata": {},
   "source": [
    "Now we have a list of DataArrays, each with the dimensions `(label_id, band, y, x)`. We'll use Scikit-Learn to train the model, which expects a 2-D array with dimensions `(observations, features)`. In this case, an \"observation\" is a single pixel (the pixel at coordinate `(-3717125, 274725)` for example), and the features are the 7 bands.\n",
    "\n",
    "So we need to reshape each DataArray from size `(1, 7, 256, 256)` to `(65536, 7)` and then concatenate them all vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57608124-e807-4786-9c41-f4d9837cebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "X = xr.concat([x.stack(pixel=(\"label_id\", \"y\", \"x\")).T for x in Xs], dim=\"pixel\")\n",
    "y = xr.concat([y.stack(pixel=(\"label_id\", \"y\", \"x\")) for y in ys], dim=\"pixel\")\n",
    "assert X.indexes[\"pixel\"].equals(y.indexes[\"pixel\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5978086-477a-4458-ac4f-81f22b7f9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4db34-5969-4236-884a-8119cc124b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068fb84f-23ed-4571-817e-739f667ca1a5",
   "metadata": {},
   "source": [
    "Thanks to xarray's indexing, we can easily go from these stacked DataArray back to a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7cf3e5-5252-4dd1-96a1-51f57c76cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = label_items[0].id\n",
    "X.sel(label_id=label_id).unstack().sel(band=\"B04\").plot(cmap=\"Reds\", figsize=(12, 9));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc15f80-1b0f-4890-83a8-ce2ed18d7a61",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Now that we've done all the pre-processing, we can train the actual model.\n",
    "\n",
    "We'll start with a scikit-learn KNeighborsClassfier ([User Guide](https://scikit-learn.org/stable/modules/neighbors.html#classification), [API Reference](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)) to establish a baseline model for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.neighbors\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1024bdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47015616-9d34-4b0b-8e13-670b0cbfb59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.neighbors.KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a85f52-8d01-4ab4-aa18-5782208531be",
   "metadata": {},
   "source": [
    "Training score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b112c6-1342-4e91-92a4-3909d467b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_train[::100], y_train[::100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be8bca2-b417-41af-b548-d612aa095781",
   "metadata": {},
   "source": [
    "Test score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c414d3de-1b51-4e3c-b506-c6896c2611e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test[::100], y_test[::100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273eb561-5249-4a36-aa49-a31508ccd688",
   "metadata": {},
   "source": [
    "Plot the first field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27fa38-683f-48ee-8238-ac50807fa8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X.sel(label_id=label_id)\n",
    "yhat = clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613035de-ebe2-44d8-bb6a-eda1827337ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "\n",
    "ys[0].plot(x=\"x\", y=\"y\", cmap=\"tab10\", ax=ax1, add_colorbar=False)\n",
    "ax2.imshow(yhat.reshape(256, 256), cmap=\"tab10\")\n",
    "plt.tight_layout()\n",
    "\n",
    "ax1.set_axis_off()\n",
    "ax2.set_axis_off()\n",
    "\n",
    "ax1.set(title=\"Actual\")\n",
    "ax2.set(title=\"Predicted\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c89261-f766-4fcb-a735-4a130e00837a",
   "metadata": {},
   "source": [
    "So we seems to be able to differentiate \"field\" from \"not a field\", but do a bad job of predicting the actual crop type. Plenty of room for improvement.\n",
    "\n",
    "## Recap\n",
    "\n",
    "We were able to train a basic ML model to predict crop types from Sentinel-2 satellite imagery. We used STAC to find and load our data, xarray to reshape the data into an appropriate form for the model, and scikit-learn to train the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1792dda3292141c6b54a76ec61e737eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "layout": "IPY_MODEL_f741701f6a0b4b46836126f9bf10ca7e"
      }
     },
     "f741701f6a0b4b46836126f9bf10ca7e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
